{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization For Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import *\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ddn.node import *\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RidgeNode(AbstractDeclarativeNode):\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super().__init__(1, n)\n",
    "        \n",
    "    # x is miu\n",
    "    # y is [beta_1^T,beta_2^T,...,beta_K^T]\n",
    "    def objective(self, x, y):\n",
    "        sum = 0\n",
    "        for i in range(K):\n",
    "            sum += 0.5 * np.sum(np.square(np.dot(A_train_splitted[i],y[i])-b_train_splitted[i])) + 0.5 * x * np.linalg.norm(y[i],ord = 2)\n",
    "        return sum\n",
    "\n",
    "    def solve(self, x):\n",
    "        # TODO\n",
    "        result = np.array([])\n",
    "        for i in range(K):\n",
    "            if i == 0:\n",
    "                result = sci.linalg.solve(np.dot(A_train_splitted[i].T,A_train_splitted[i]) + x * I_out, np.dot(A_train_splitted[i].T, b_train_splitted[i]))\n",
    "            else:\n",
    "                result = np.vstack((result, sci.linalg.solve(np.dot(A_train_splitted[i].T,A_train_splitted[i]) + x * I_out, np.dot(A_train_splitted[i].T,b_train_splitted[i]))))\n",
    "            # print(result.shape)\n",
    "        return result, None\n",
    "\n",
    "    def gradient(self, x, y=None, ctx=None):\n",
    "        # TODO\n",
    "        if y is None:\n",
    "            y, ctx = self.solve(x)\n",
    "        \n",
    "        result = np.array([])\n",
    "        for i in range(K):\n",
    "            if i == 0:\n",
    "                result = (-1) * sci.linalg.solve(np.dot(A_train_splitted[i].T,A_train_splitted[i]) + x * I_out, y[i])\n",
    "            else:\n",
    "                result = np.vstack((result, (-1) * sci.linalg.solve(np.dot(A_train_splitted[i].T,A_train_splitted[i]) + x * I_out, y[i])))\n",
    "            # print(result.shape)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_objective(y, A_test_splitted, b_test_splitted):\n",
    "    sum = 0\n",
    "    for i in range(K):\n",
    "        y_k = y[i]\n",
    "        sum += (0.5) * np.sum(np.square(np.dot(A_test_splitted[i], y_k) - b_test_splitted[i]))\n",
    "    return sum\n",
    "\n",
    "\n",
    "def function_objective_one(y, A_test_splitted, b_test_splitted):\n",
    "    return (0.5) * np.sum(np.square(np.dot(A_test_splitted, y) - b_test_splitted))\n",
    "\n",
    "\n",
    "def derivative_objective_y(y, A_test_splitted, b_test_splitted):\n",
    "    result = np.array([])\n",
    "    for i in range(K):\n",
    "        if i == 0:\n",
    "            result = np.dot((np.dot(y[i].T, A_test_splitted[i].T) - b_test_splitted[i].T), A_test_splitted[i])\n",
    "        else:\n",
    "            result = np.vstack((result, np.dot((np.dot(y[i].T, A_test_splitted[i].T) - b_test_splitted[i].T), A_test_splitted[i])))\n",
    "        # print(result.shape)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGradientDescent(node, miu_init, b_test_splitted, A_test_splitted, step_size=1.0e-1, tol=1.0e-8, max_iters=1500000, verbose=False):\n",
    "    # Initialize\n",
    "    cnt = 0\n",
    "    miu = miu_init\n",
    "    all_miu = []\n",
    "    gradient = []\n",
    "    axis_x = []\n",
    "    history = []\n",
    "    loss = []\n",
    "\n",
    "    # Iteration\n",
    "    for i in range(max_iters):\n",
    "        # solve the lower-level problem and compute the upper-level objective\n",
    "        y, _ = node.solve(miu)\n",
    "        history.append(function_objective(y, A_test_splitted, b_test_splitted))\n",
    "        if verbose: print(\"{:5d}: {}\".format(i, history[-1]))\n",
    "        if (len(history) > 2) and (history[-2] - history[-1]) < tol:\n",
    "            print(y)\n",
    "            print(function_objective(y, A_test_splitted, b_test_splitted))\n",
    "            break\n",
    "        \n",
    "        # print(derivative_objective_y(y, A_test_splitted, b_test_splitted).shape)\n",
    "        # compute the gradient of the upper-level objective with respect to x via the chain rule\n",
    "        dJdx = 0 \n",
    "        for j in range(K):\n",
    "            # print(derivative_objective_y(y, A_test_splitted, b_test_splitted)[j].shape)\n",
    "            # print(node.gradient(miu, y)[j].shape)\n",
    "            dJdx += np.dot((derivative_objective_y(y, A_test_splitted, b_test_splitted)[j]).T, node.gradient(miu, y)[j])\n",
    "\n",
    "        if i == 0:\n",
    "            print(node.gradient(miu, y))\n",
    "            print(node.gradient(miu, y).reshape((out*K,1)))\n",
    "            print(y)\n",
    "            \n",
    "        # take a step in the negative gradient direction\n",
    "        miu -= step_size * dJdx\n",
    "        all_miu.append(miu)\n",
    "        gradient.append(abs(dJdx))\n",
    "        cnt += 1\n",
    "        axis_x.append(cnt)\n",
    "        loss.append(function_objective(y, A_test_splitted, b_test_splitted) / (K * A_test_splitted[0].shape[0]))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(axis_x, all_miu)\n",
    "    plt.xlabel(\"iteration time\")\n",
    "    plt.ylabel(\"hyperparameter mu\")\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(all_miu, gradient)\n",
    "    plt.xlabel(\"hyperparameter mu\")\n",
    "    plt.ylabel(\"gradient\")\n",
    "    print(gradient[-1])\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(all_miu, loss)\n",
    "    plt.xlabel(\"hyperparameter mu\")\n",
    "    plt.ylabel(\"loss function\")\n",
    "    print(loss[-1])\n",
    "\n",
    "    print(\"cnt: \", cnt)\n",
    "    return miu, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DataSet\n",
    "iris = datasets.load_iris()\n",
    "A = iris.data  \n",
    "b = iris.target  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "A_scaled = scaler.fit_transform(A)\n",
    "\n",
    "# K-fold\n",
    "K = 5\n",
    "KF = KFold(n_splits = K)\n",
    "\n",
    "A_train_splitted = []\n",
    "A_test_splitted = []\n",
    "b_train_splitted = []\n",
    "b_test_splitted = []\n",
    "\n",
    "for train_index, test_index in KF.split(A_scaled):\n",
    "    A_train, A_test = A_scaled[train_index], A_scaled[test_index]\n",
    "    A_train_splitted.append(A_train)\n",
    "    A_test_splitted.append(A_test)\n",
    "    b_train, b_test = b[train_index], b[test_index]\n",
    "    b_train_splitted.append(b_train)\n",
    "    b_test_splitted.append(b_test)\n",
    "    print(\"X_train: \", A_train.shape)\n",
    "    print(\"X_test: \", A_test.shape)\n",
    "    \n",
    "out = A_train_splitted[0].shape[1]\n",
    "I_out = np.eye(out)\n",
    "\n",
    "node = RidgeNode(K*out)\n",
    "miu_init = 800\n",
    "x, history_gd = simpleGradientDescent(node, miu_init, b_test_splitted, A_test_splitted)\n",
    "y_star, _ = node.solve(x)\n",
    "print(\"Gradient descent to give x = {}\".format(x))\n",
    "# Visualization\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
